{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Accounting for error in simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore some of the ways that error can enter into a model, and methods for handling it. The major forms of error and uncertainty are:\n",
    "1. Observational error. \n",
    "2. Parameter uncertainty.\n",
    "3. Prediction uncertainty.\n",
    "4. Structural error.\n",
    "\n",
    "Each is demonstrated by it's own code snippet below for the case of a simple linear model with two parameters: gradient, $m_0$, and $y$-intercept, $c_0$. The model is denoted \n",
    "\n",
    "$$y = mx+c = f(x;\\boldsymbol{\\theta}),$$ \n",
    "\n",
    "where $\\boldsymbol{\\theta} = [m,c]$ is the parameter vector, and $\\boldsymbol{\\theta}_0$ its true value we are trying to determine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.1 Observational error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concede that no instrument makes perfect measurements, and therefore there is some amount of random error, $\\epsilon$, in an observation, i.e., $ \\tilde{y}_i = y_i+\\epsilon$, where $y_i$ is the true value of a quantity we are trying to measure, and $\\tilde{y_i}$ is the actual measurement made.\n",
    "\n",
    "We assume that $\\epsilon$ is a normally distributed random variable with variance, $\\sigma_i^2$.\n",
    "\n",
    "For the simple linear model, we demonstrate the \"observation\" process by sampling the true model, $f(x;\\boldsymbol{\\theta}_0)$, at the observation points, $x_i$, and adding normally distributed random error. \n",
    "\n",
    "Execute the code below to see how the observation points (open circles) deviate from the true model (blue line). You can also plot the best-fit model.\n",
    "\n",
    "***Is the best-fit model the same as the \"true\" model?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import modules\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from ipywidgets import interact \n",
    "\n",
    "# define a model\n",
    "ts = 14.\n",
    "x = np.linspace(0,1,101)\n",
    "\n",
    "# model parameters\n",
    "m0 = 2.       # true gradient\n",
    "c0 = 3.       # true intercept\n",
    "v = 0.15      # variance of random error when making measurements\n",
    "\n",
    "# define the model, a simple linear function\n",
    "def func(x,m,c): \n",
    "    return m*x+c\n",
    "\n",
    "# define the error, a random draw from a normal distribution\n",
    "def e(x,v): \n",
    "    return np.random.randn(len(x))*np.sqrt(v)\n",
    "\n",
    "# compute the \"true\" model, using the \"true\" parameters\n",
    "y = func(x,m0,c0)\n",
    "\n",
    "# seed the random number generator so we get the same numbers each time\n",
    "np.random.seed(13)\n",
    "\n",
    "# define some values of the independent variable at which we will be taking our \"observations\"\n",
    "xo = np.linspace(0,1,12)[1:-1]\n",
    "\n",
    "# compute the observations - \"true\" model + random error (drawn from normal distribution)\n",
    "yo = func(xo,m0,c0) + e(xo,v)\n",
    "\n",
    "def plot_observations(N_obs, true_model, RMS_fit, error_dist):\n",
    "        # check N_obs does not exceed length of observations\n",
    "    i = np.min([len(xo), N_obs])\n",
    "        # initialize figure window and axes\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "    fig.set_size_inches([8.,5.])\n",
    "        # plot the \"true\" model    \n",
    "    if true_model:\n",
    "        ln1 = ax.plot(x,y,'b-', label = 'true model',zorder = 10)\n",
    "    else:\n",
    "        ln1 = []\n",
    "        # plot the observations\n",
    "    ln2 = ax.plot(xo[:i],yo[:i],'wo', mec = 'k', mew = 1.5, ms = 5, label = r'observations', zorder = 10)\n",
    "        # add \"best-fit\" model if appropriate\n",
    "    if RMS_fit:\n",
    "        # find best-fit model\n",
    "        p2,pc = curve_fit(func, xo[:i], yo[:i], [1,1])\n",
    "        # plot model\n",
    "        ax.plot(x,func(x,*p2),'r-')\n",
    "    \n",
    "        # add normal distributions\n",
    "    ylim = ax.get_ylim()\n",
    "    ye = np.linspace(-ylim[1],ylim[1],101)*0.2\n",
    "    ye2 = np.linspace(-ylim[1],ylim[1],101)*0.25\n",
    "        # loop over plotted observations\n",
    "    for xoi, yoi in zip(xo[:i],yo[:i]):\n",
    "            # normal dist\n",
    "        xi = 0.05*np.exp(-(ye)**2/v)+xoi\n",
    "            # add to plot\n",
    "        if error_dist:\n",
    "            ax.plot(xi, ye+func(xoi,m0,c0), 'k-', lw = 0.5, zorder = 0)\n",
    "            ax.plot(xi*.0+xoi, ye2+func(xoi,m0,c0), '-', lw = 0.5, zorder = 0, color = [0.5, 0.5, 0.5])\n",
    "\n",
    "        # plot upkeep + legend\n",
    "    ax.set_xlim(ax.get_xlim())\n",
    "    lns = ln1+ln2\n",
    "    lbs = [ln.get_label() for ln in lns]\n",
    "    ax.legend(lns,lbs,loc = 2,prop={'size':ts})\n",
    "    ax.set_ylim([1,7])\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_xlabel('x',size = ts)\n",
    "    ax.set_ylabel('y',size = ts)\n",
    "    for t in ax.get_xticklabels()+ax.get_yticklabels(): t.set_fontsize(ts)\n",
    "    plt.show()\n",
    "    \n",
    "interact(plot_observations, N_obs = (2,10,1), true_model = True, RMS_fit = False, error_dist = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.2 Parameter uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2.1 Grid search parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use observations to learn about parameters during model calibration. However, if the observations are uncertain, then so too must be the parameters.\n",
    "\n",
    "One way to describe parameter uncertainty is via a *probability distribution*. When that distribution has been informed by the observations, we call it the *posterior* and denote it $P(\\boldsymbol{\\theta}|\\tilde{y}_i)$.\n",
    "\n",
    "For a uniform *prior distribution* (our initial expectations about the parameter, before seeing any data), and normally distributed errors, we can compute the posterior directly.\n",
    "\n",
    "$$ r(\\boldsymbol{\\theta}) = -\\sum_i^n \\frac{1}{\\sigma_i^2}(\\tilde{y}_i-f(x_i;\\boldsymbol{\\theta}))^2, \\quad\\quad\\quad\\quad P(\\boldsymbol{\\theta}|\\tilde{y}_i) = A \\exp(r/2),$$\n",
    "\n",
    "where $r$ is the objective function to be maximized during calibration, and $A$ is a normalizing constant.\n",
    "\n",
    "We compute $r(\\boldsymbol{\\theta})$ by performing a grid search over parameter space, running the model and computing the objective function at discrete points, $\\boldsymbol{\\theta}_i$.\n",
    "\n",
    "Execute the code below to see a plot of the posterior over parameter space. Use the slider bars to manually adjust the parameters of an arbitrary model, and see how the fit of this model to the data (left plot) corresponds to different likelihoods of the posterior (right).\n",
    "\n",
    "***Does the true model correspond to the maximum of the posterior? Explain.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate parameter grid for grid search\n",
    "m = np.linspace(m0-2,m0+2,31); dm = m[1]-m[0]\n",
    "c = np.linspace(c0-1,c0+1,31); dc = c[1]-c[0]\n",
    "M,C = np.meshgrid(m,c)\n",
    "\n",
    "# compute objective function\n",
    "    # empty vector, correct size, for storing computed objective function\n",
    "r = 0.*M.flatten() \n",
    "    # for each parameter combination in the grid search\n",
    "for i,theta in enumerate(zip(M.flatten(), C.flatten())):\n",
    "        # unpack parameter vector\n",
    "    mi,ci = theta\n",
    "        # compute objective function\n",
    "    r[i]=-np.sum((yo-func(xo,mi,ci))**2)/v\n",
    "    # reshape objective function to meshgrid dimensions\n",
    "R = np.array(r).reshape([len(c), len(m)])\n",
    "    # compute posterior\n",
    "post = np.exp(R/2.)\n",
    "    # convert 2D mesh to vectors\n",
    "mv, cv, pv = [vi.flatten() for vi in [M,C,post]]\n",
    "    \n",
    "# plotting function\n",
    "def plot_posterior(m1,c1):\n",
    "        # initialize figure window and axes\n",
    "    fig = plt.figure(figsize=[16.,6.5])\n",
    "    ax1 = plt.axes([0.15,0.15,0.35,0.75])\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    \n",
    "    # show data and fitted models\n",
    "        # plot the \"true\" model    \n",
    "    ln1 = ax1.plot(x,y,'b-', label = 'true model',zorder = 10)\n",
    "        # plot the observations\n",
    "    ln2 = ax1.plot(xo,yo,'wo', mec = 'k', mew = 1.5, ms = 5, label = r'observations', zorder = 10)\n",
    "        # best-fit model\n",
    "    p2,pc = curve_fit(func, xo, yo, [1,1])\n",
    "    ln3 = ax1.plot(x,func(x,*p2),'r-', label = 'best-fit model')\n",
    "        # show model with [m1,c1]\n",
    "    i,j = np.argmin(abs(m-m1)), np.argmin(abs(c-c1))    # snap to nearest grid value\n",
    "    ln4 = ax1.plot(x,func(x,m[i],c[j]),'g-', label = 'aribtrary model: r=%3.2f'%R[i,j])\n",
    "    lns = ln1+ln2+ln3+ln4\n",
    "    lbs = [ln.get_label() for ln in lns]\n",
    "    ax1.legend(lns,lbs,loc=2)\n",
    "    \n",
    "    # show posterior\n",
    "        # plot posterior as surface over 2D parameter space\n",
    "    ax2.plot_surface(M, C, post, rstride=1, cstride=1,cmap=cm.Oranges, lw = 0.5, zorder = 10)\n",
    "    \n",
    "    # show models on posterior\n",
    "        # best-fit model\n",
    "    im = np.argmax(pv)\n",
    "    ax2.plot3D([mv[im],],[cv[im],],[pv[im],],'ro',mec = 'r', ms = 7)\n",
    "        # true model\n",
    "    ax2.plot3D([m0,],[c0,],[np.exp(-np.sum((yo-func(xo,m0,c0))**2)/(2.*v)),],'bo',mec = 'b', ms = 7)\n",
    "        # arbitrary model\n",
    "    im = np.argmax(pv)\n",
    "    ax2.plot3D([M[j,i],],[C[j,i],],[post[j,i],],'go',mec = 'g', ms = 7)\n",
    "\n",
    "    # plot upkeep, labels\n",
    "    ax2.set_xlabel('m',size = ts)\n",
    "    ax2.set_ylabel('c',size = ts)\n",
    "    ax2.set_xticklabels([])\n",
    "    ax2.set_yticklabels([])\n",
    "    ax2.set_zticklabels([])\n",
    "    ax1.set_xlabel('x',size = ts)\n",
    "    ax1.set_ylabel('y',size = ts)\n",
    "    for ax in [ax1,ax2]:\n",
    "        for t in ax.get_xticklabels()+ax.get_yticklabels(): t.set_fontsize(ts)\n",
    "        \n",
    "    # plot view angle\n",
    "    ax2.view_init(45, 15-90)\n",
    "    plt.show()\n",
    "    \n",
    "interact(plot_posterior, m1 = (m0-2,m0+2,4./51), c1 = (c0-1,c0+1,2./51))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2.2 Monte-Carlo parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method for estimating posterior parameter distributions is Markov-Chain Monte-Carlo. The mathematics of the method are well beyond the scope of this notebook (but there are plenty of excellent descriptions online). \n",
    "\n",
    "There is a excellent Python package called [emcee](http://dan.iel.fm/emcee/current/) that implements a number of MCMC algorithms. We will use the package to replicate the grid search above.\n",
    "\n",
    "If you are using the Anaconda Python distribution, then emcee can be installed from the command line using `pip`\n",
    "\n",
    "`pip install emcee`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "# log likelihood for the model, given the data\n",
    "def lnprob(pars, obs):\n",
    "    v = 0.15\n",
    "    return -np.sum((obs[:,1]-func(obs[:,0],*pars))**2)/v\n",
    "    \n",
    "ndim = 2                # parameter space dimensionality\n",
    "nwalkers=10             # number of walkers\n",
    "\n",
    "# create the emcee object (set threads>1 for multiprocessing)\n",
    "data = np.array([xo,yo]).T\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, threads=1, args=[data,])\n",
    "\n",
    "# set the initial location of the walkers\n",
    "pars = [2.5, 2.5]       # initial guess\n",
    "p0 = np.array([pars + 1e-3*np.random.randn(ndim) for i in range(nwalkers)])  # add some noise\n",
    "\n",
    "# set the emcee sampler to start at the initial guess and run 100 burn-in jumps\n",
    "pos,prob,state=sampler.run_mcmc(p0,100)\n",
    "sampler.reset()\n",
    "\n",
    "# run 1000 jumps and save the results \n",
    "pos,prob,state=sampler.run_mcmc(pos,1000)\n",
    "f = open(\"chain.dat\", \"w\")\n",
    "nk,nit,ndim=sampler.chain.shape\n",
    "for k in range(nk):\n",
    "    for i in range(nit):\n",
    "        f.write(\"{:d} {:d} \".format(k, i))\n",
    "        for j in range(ndim):\n",
    "            f.write(\"{:15.7f} \".format(sampler.chain[k,i,j]))\n",
    "        f.write(\"{:15.7f}\\n\".format(sampler.lnprobability[k,i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the corner module to display some of the results. This can be installed by `pip` in the same way as `emcee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show corner plot of the results\n",
    "import corner\n",
    "chain = np.genfromtxt('chain.dat')\n",
    "weights = chain[:,-1]\n",
    "weights -= np.max(weights)\n",
    "weights = np.exp(weights)\n",
    "labels = ['m','c']\n",
    "fig = corner.corner(chain[:,2:-1], labels=labels, weights=weights, smooth=1, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.3 Prediction uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establishing the posterior gives us a quantitative measure of which parameter combinations are more likely than others. To make a forecast of the future (a probabilistic description of all outcomes), we sample parameter combinations from the posterior and use the model to make predictions for these. The key is that predictions corresponding to the more likely parameter combinations will have a greater weight in the final forecast.\n",
    "\n",
    "Execute the code below. Use the slider bar to take more and more samples from the posterior. Note how the forecast - a histogram of predictions - starts to approximate a probability distribution as more and more samples are drawn. \n",
    "\n",
    "***How well does the 90% forecast interval approximate the true model?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct covariance matrix\n",
    "    # means\n",
    "pv = pv/np.sum(pv)\n",
    "m1 = np.sum(pv*mv)\n",
    "c1 = np.sum(pv*cv)\n",
    "    # variances\n",
    "smm = np.sum(pv*(mv-m1)**2)\n",
    "scc = np.sum(pv*(cv-c1)**2)\n",
    "scm = np.sum(pv*(mv-m1)*(cv-c1))\n",
    "    # matrix\n",
    "cov = np.array([[smm,scm],[scm,scc]])\n",
    "\n",
    "# plotting function\n",
    "def plot_ensemble(logN,predict):\n",
    "        # initialize figure window and axes\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.set_size_inches([16.,5.])\n",
    "    \n",
    "        # vector for long range forecast\n",
    "    if predict:\n",
    "        x2 = np.linspace(0,3.2, 101)\n",
    "        ix = np.argmin(abs(x2 - 3.0))\n",
    "    else:\n",
    "        x2 = np.linspace(0,1,101)\n",
    "\n",
    "        # plot the \"true\" model  \n",
    "    ln1 = axs[0].plot(x2,func(x2,m0,c0),'b-', label = 'true model',zorder = 10)\n",
    "        # plot the observations\n",
    "    ln2 = axs[0].plot(xo,yo,'wo', mec = 'k', mew = 1.5, ms = 5, label = r'observations', zorder = 10)\n",
    "    \n",
    "        # take samples from multivariate normal\n",
    "    np.random.seed(13)\n",
    "    samples = multivariate_normal.rvs(mean = [m1, c1], cov = cov, size = 2**logN)\n",
    "    if logN == 0: samples = [samples,]\n",
    "        # plot line for each sample      \n",
    "    prediction = []\n",
    "    for i,s in enumerate(samples):\n",
    "            # plot sample model\n",
    "        if i==0:\n",
    "            ln3 = axs[0].plot(x2,func(x2,*s),'k-', zorder = 0, lw = 0.5, label = 'sample model')\n",
    "        else:\n",
    "            axs[0].plot(x2,func(x2,*s),'k-', zorder = 0, lw = 0.5, alpha = 1./np.sqrt(2**logN))\n",
    "            # save prediction of sample model\n",
    "        if predict:\n",
    "            prediction.append(func(x2[ix],*s))\n",
    "        \n",
    "        # x-limits: choice to show prediction\n",
    "    if not predict:\n",
    "        axs[0].set_xlim([0,1])\n",
    "    else:\n",
    "        axs[0].set_xlim([0,3.2])\n",
    "        ylim = axs[0].get_ylim(); axs[0].set_ylim(ylim)\n",
    "        axs[0].plot([3,3],ylim, 'k--')\n",
    "        \n",
    "        # plot histogram of predictions\n",
    "    if predict:\n",
    "        h,e = np.histogram(prediction, bins = np.linspace(6,14,19))\n",
    "        axs[1].bar(e[:-1], h, e[1]-e[0], color = [0.5,0.5,0.5])\n",
    "        pcs = np.percentile(prediction, [5,95])\n",
    "        ylim = axs[1].get_ylim(); axs[1].set_ylim(ylim)\n",
    "        axs[1].plot([func(x2[ix],m0,c0), func(x2[ix],m0,c0)],ylim,'b-')\n",
    "        axs[1].plot([pcs[0],pcs[0]],ylim,'k--')\n",
    "        axs[1].plot([pcs[1],pcs[1]],ylim,'k--')\n",
    "        \n",
    "    # legend\n",
    "    lns = ln1+ln2+ln3\n",
    "    lbs = [ln.get_label() for ln in lns]\n",
    "    axs[0].legend(lns,lbs,loc=2)\n",
    "        \n",
    "    # plot upkeep\n",
    "    axs[0].set_xlabel('x',size = ts)\n",
    "    axs[0].set_ylabel('y',size = ts)\n",
    "    axs[1].set_xlabel('prediction at x=3',size = ts)\n",
    "    axs[1].set_ylabel('frequency',size = ts)\n",
    "    for ax in axs:\n",
    "        for t in ax.get_xticklabels()+ax.get_yticklabels(): t.set_fontsize(ts)\n",
    "            \n",
    "    plt.show()\n",
    "    \n",
    "interact(plot_ensemble, logN = (0,10,1), predict = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.4 Structural (or model) error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This refers to deficiencies in the construction of the model, which could include: the physics that are represented, discretization of the equations, assumptions made about homogeneity or features that are excluded.\n",
    "\n",
    "In this example, we will consider structural error introduced by incorrect physics. When physical laws are used to formulate governing equations for a model, the result is a class of functions, $f(\\cdot;\\boldsymbol{\\theta})$, whose parameters determine the behaviour of the model under particular circumstances. It is entirely possible to use the wrong parameters, $\\boldsymbol{\\theta}$, to try and predict the future: this is *parameter error*.\n",
    "\n",
    "However, it is also possible that our interpretation of the relevant physics was incorrect and, actually, a different class of functions, $g(\\cdot;\\boldsymbol{\\theta})$, provides a more appropriate description of the phenomenon we are attempting to model. We call this *structural (or model) error*.\n",
    "\n",
    "In the examples presented here, we \"know\" the underlying model is linear in nature. However, let's assume that we mistakenly thought it was in fact logarithmic, i.e.,\n",
    "\n",
    "$$y = a \\ln(x_i-x_0)+b,$$\n",
    "\n",
    "where $a$, $x_0$ and $b$ are the three parameters of this different model. Executing the code below will run through the same process in the cells above: a grid search is undertaken to find the objective function, and hence posterior, the posterior is sampled from to provide a forecast of the future. Compare how the two models' forecasts of the future differ and how they agree with the true outcome.\n",
    "\n",
    "***Which model provides a better fit to the data?***\n",
    "\n",
    "***How do the forecasts of the two models differ?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vectors for making predictions\n",
    "x2 = np.linspace(0.01,3.2, 101)\n",
    "ix = np.argmin(abs(x2 - 3.0))\n",
    "\n",
    "N = 2**10\n",
    "\n",
    "# linear model samples\n",
    "np.random.seed(13)\n",
    "LNsamples = multivariate_normal.rvs(mean = [m1, c1], cov = cov, size = N)\n",
    "\n",
    "# log model samples\n",
    "    # define log model, three parameters\n",
    "def func2(x,*p): \n",
    "    return p[0]*np.log(x+p[1])+p[2]\n",
    "    # find best-fit log model\n",
    "p2,pc = curve_fit(func2, xo, yo, [1,1,1])\n",
    "    # construct parameter search grid\n",
    "pi = np.linspace(p2[0]/10.,p2[0]*10.,51); dpi = pi[1]-pi[0]\n",
    "pj = np.linspace(p2[1]/10.,p2[1]*10.,51); dpj = pj[1]-pj[0]\n",
    "pk = np.linspace(p2[2]/10.,p2[2]*10.,51); dpk = pk[1]-pk[0]\n",
    "PI, PJ, PK = np.meshgrid(pi,pj,pk)\n",
    "# compute posterior for log model\n",
    "    # empty vector, correct size, for storing computed objective function\n",
    "r2 = 0.*PI.flatten() \n",
    "    # for each parameter combination in the grid search\n",
    "for i,theta in enumerate(zip(PI.flatten(), PJ.flatten(), PK.flatten())):\n",
    "        # compute objective function\n",
    "    r2[i]=-np.sum((yo-func2(xo,*theta))**2)/v\n",
    "    # reshape objective function to meshgrid dimensions\n",
    "R2 = np.array(r2).reshape([len(pi), len(pj), len(pk)])\n",
    "    # compute posterior\n",
    "post2 = np.exp(R2/2.)\n",
    "    # convert 2D mesh to vectors\n",
    "pi,pj,pk,pv2 = [vi.flatten() for vi in [PI,PJ,PK,post2]]\n",
    "# compute covariance matrix for log model\n",
    "    # normalize posterior\n",
    "pv2 = pv2/(np.sum(pv2))\n",
    "    # means\n",
    "pi1 = np.sum(pv2*pi)\n",
    "pj1 = np.sum(pv2*pj)\n",
    "pk1 = np.sum(pv2*pk)\n",
    "    # variances\n",
    "sii = np.sum(pv2*(pi-pi1)**2)\n",
    "sjj = np.sum(pv2*(pj-pj1)**2)\n",
    "skk = np.sum(pv2*(pk-pk1)**2)\n",
    "sij = np.sum(pv2*(pi-pi1)*(pj-pj1))\n",
    "sik = np.sum(pv2*(pi-pi1)*(pk-pk1))\n",
    "sjk = np.sum(pv2*(pk-pk1)*(pj-pj1))\n",
    "    # assemble matrix\n",
    "cov2 = np.array([[sii,sij, sik],[sij, sjj, sjk],[sik, sjk, skk]])\n",
    "    \n",
    "np.random.seed(13)\n",
    "LGsamples = multivariate_normal.rvs(mean = [pi1, pj1, pk1], cov = cov2, size = N)\n",
    "\n",
    "# plotting function\n",
    "def plot_structural(LNmodel, LNset, LGmodel, LGset):\n",
    "        # initialize figure window and axes\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.set_size_inches([16.,5.])\n",
    "        # plot the observations\n",
    "    axs[0].plot(xo,yo,'wo', mec = 'k', mew = 1.5, ms = 5, zorder = 10)\n",
    "            \n",
    "    # plot best-fit log model\n",
    "    if LGmodel:\n",
    "        axs[0].plot(x2,func2(x2,*p2),'g-',lw = 2)\n",
    "        axs[0].plot(x2,func2(x2,*p2),'k--',lw = 2)\n",
    "        \n",
    "    # plot ensemble for best-fit log model\n",
    "    if LGset:\n",
    "        # plot line for each sample\n",
    "        prediction = []\n",
    "        for s in LGsamples: \n",
    "            axs[0].plot(x2, func2(x2,*s), 'g-', alpha = 1./np.sqrt(N))\n",
    "            prediction.append(func2(x2[ix],*s))\n",
    "        # plot histogram of predictions\n",
    "        h,e = np.histogram(prediction, bins = np.linspace(4,14,33))\n",
    "        axs[1].bar(e[:-1], h, e[1]-e[0], color = 'g', alpha = 0.5)\n",
    "        pcs = np.percentile(prediction, [5,95])\n",
    "        ylim = axs[1].get_ylim(); axs[1].set_ylim(ylim)\n",
    "        axs[1].plot([func(x2[ix],m0,c0), func(x2[ix],m0,c0)],ylim,'b-')\n",
    "        axs[1].plot([pcs[0],pcs[0]],ylim,'g--')\n",
    "        axs[1].plot([pcs[1],pcs[1]],ylim,'g--')\n",
    "    \n",
    "    # plot best-fit linear model\n",
    "    if LNmodel:   \n",
    "        p1,pc = curve_fit(func, xo, yo, [1,1])\n",
    "        axs[0].plot(x2,func(x2,*p1),'r-',lw = 2)\n",
    "        axs[0].plot(x2,func(x2,*p1),'k--',lw = 2)\n",
    "    \n",
    "    # plot ensemble for linear model\n",
    "    if LNset:\n",
    "        # plot line for each sample\n",
    "        prediction = []\n",
    "        for s in LNsamples: \n",
    "            axs[0].plot(x2, func(x2,*s), 'r-', alpha = 1./np.sqrt(N))\n",
    "            prediction.append(func(x2[ix],*s))\n",
    "        # plot histogram of predictions\n",
    "        h,e = np.histogram(prediction, bins = np.linspace(4,14,33))\n",
    "        axs[1].bar(e[:-1], h, e[1]-e[0], color = 'r', alpha = 0.5)\n",
    "        pcs = np.percentile(prediction, [5,95])\n",
    "        ylim = axs[1].get_ylim(); axs[1].set_ylim(ylim)\n",
    "        axs[1].plot([func(x2[ix],m0,c0), func(x2[ix],m0,c0)],ylim,'b-')\n",
    "        axs[1].plot([pcs[0],pcs[0]],ylim,'r--')\n",
    "        axs[1].plot([pcs[1],pcs[1]],ylim,'r--')\n",
    "    \n",
    "    # axis lims\n",
    "    if LNset or LGset: \n",
    "        axs[0].set_xlim([0,3.2])\n",
    "        ylim = axs[0].get_ylim()\n",
    "    else: \n",
    "        axs[0].set_xlim([0,1])\n",
    "        axs[0].set_ylim([1,6])\n",
    "        \n",
    "    # plot upkeep\n",
    "    axs[0].set_xlabel('x',size = ts)\n",
    "    axs[0].set_ylabel('y',size = ts)\n",
    "    axs[1].set_xlabel('prediction at x=3',size = ts)\n",
    "    axs[1].set_ylabel('frequency',size = ts)\n",
    "    for ax in axs:\n",
    "        for t in ax.get_xticklabels()+ax.get_yticklabels(): t.set_fontsize(ts)\n",
    "    plt.show()\n",
    "    \n",
    "interact(plot_structural, LNmodel=False, LNset=False, LGmodel=False, LGset=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
